{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class shave_block(nn.Module):\n",
    "    def __init__(self, s):\n",
    "        super(shave_block, self).__init__()\n",
    "        self.s=s\n",
    "    def forward(self,x):\n",
    "        return x[:,:,self.s:-self.s,self.s:-self.s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LambdaBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LambdaBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, input):\n",
    "        output = []\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(input))\n",
    "        return output if output else input\n",
    "\n",
    "class Lambda(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return self.lambda_func(self.forward_prepare(input))\n",
    "\n",
    "class LambdaMap(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return list(map(self.lambda_func,self.forward_prepare(input)))\n",
    "\n",
    "class LambdaReduce(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return reduce(self.lambda_func,self.forward_prepare(input))\n",
    "\n",
    "\n",
    "G = nn.Sequential( # Sequential,\n",
    "    nn.ReflectionPad2d((40, 40, 40, 40)),\n",
    "    nn.Conv2d(1,32,(9, 9),(1, 1),(4, 4)),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32,64,(3, 3),(2, 2),(1, 1)),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64,128,(3, 3),(2, 2),(1, 1)),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Sequential( # Sequential,\n",
    "        LambdaMap(lambda x: x, # ConcatTable,\n",
    "            nn.Sequential( # Sequential,\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "            ),\n",
    "            shave_block(2),\n",
    "        ),\n",
    "        LambdaReduce(lambda x,y: x+y), # CAddTable,\n",
    "    ),\n",
    "    nn.Sequential( # Sequential,\n",
    "        LambdaMap(lambda x: x, # ConcatTable,\n",
    "            nn.Sequential( # Sequential,\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "            ),\n",
    "            shave_block(2),\n",
    "        ),\n",
    "        LambdaReduce(lambda x,y: x+y), # CAddTable,\n",
    "    ),\n",
    "    nn.Sequential( # Sequential,\n",
    "        LambdaMap(lambda x: x, # ConcatTable,\n",
    "            nn.Sequential( # Sequential,\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "            ),\n",
    "            shave_block(2),\n",
    "        ),\n",
    "        LambdaReduce(lambda x,y: x+y), # CAddTable,\n",
    "    ),\n",
    "    nn.Sequential( # Sequential,\n",
    "        LambdaMap(lambda x: x, # ConcatTable,\n",
    "            nn.Sequential( # Sequential,\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "            ),\n",
    "            shave_block(2),\n",
    "        ),\n",
    "        LambdaReduce(lambda x,y: x+y), # CAddTable,\n",
    "    ),\n",
    "    nn.Sequential( # Sequential,\n",
    "        LambdaMap(lambda x: x, # ConcatTable,\n",
    "            nn.Sequential( # Sequential,\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128,128,(3, 3)),\n",
    "                nn.BatchNorm2d(128),\n",
    "            ),\n",
    "            shave_block(2),\n",
    "        ),\n",
    "        LambdaReduce(lambda x,y: x+y), # CAddTable,\n",
    "    ),\n",
    "    nn.ConvTranspose2d(128,64,(3, 3),(2, 2),(1, 1),(1, 1)),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(64,32,(3, 3),(2, 2),(1, 1),(1, 1)),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32,2,(9, 9),(1, 1),(4, 4)),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "\n",
    "G=G.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "D = models.resnet18(pretrained=False,num_classes=2)\n",
    "D.fc = nn.Sequential(nn.Linear(2048, 1), nn.Sigmoid())\n",
    "D = D.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2yuv,yuv2rgb\n",
    "\n",
    "class img_data(data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        files = os.listdir(path)\n",
    "        self.files = [os.path.join(path,x) for x in files]\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        img = Image.open(self.files[index])\n",
    "        yuv = rgb2yuv(img)\n",
    "        y = yuv[...,0]-0.5\n",
    "        u_t = yuv[...,1] / 0.43601035\n",
    "        v_t = yuv[...,2] / 0.61497538\n",
    "\n",
    "\n",
    "        return torch.Tensor(np.expand_dims(y,axis=0)),torch.Tensor(np.stack([u_t,v_t],axis=0))\n",
    "trainset = img_data('train')\n",
    "valset = img_data('/data/data/coco/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "params = {'batch_size': 20,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "training_generator = data.DataLoader(trainset, **params)\n",
    "validation_generator = data.DataLoader(valset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "p=Image.open('5.jpg').convert('RGB').resize((256,256))\n",
    "img_yuv = rgb2yuv(p)\n",
    "infimg= img_yuv[...,0].reshape(1,1,256,256)\n",
    "img_variable = Variable(torch.Tensor(infimg-0.5)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "i=0\n",
    "for epoch in range(1):\n",
    "    for y, uv in training_generator:\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.Tensor(y.size(0), 1).fill_(1.0), requires_grad=False).cuda()\n",
    "        fake = Variable(torch.Tensor(y.size(0), 1).fill_(0.0), requires_grad=False).cuda()\n",
    "\n",
    "        # Configure input\n",
    "        yvar = Variable(y).cuda()\n",
    "        real_imgs = torch.cat([yvar,Variable(uv).cuda()],dim=1)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        gen_imgs = torch.cat([yvar.detach(),G(yvar)],dim=1)\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(D(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            print (\"[D loss: %f]\" % (d_loss.item()))\n",
    "        if i%2000==0:\n",
    "            torch.save(D.state_dict(), 'Dinit.pth')\n",
    "            break\n",
    "'''\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('weight.json','w') as k:\n",
    "    json.dump(j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=torch.load('weights/G3.pth')\n",
    "for i in j.keys():\n",
    "    j[i]=j[i].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.load_state_dict(torch.load('weights/G3.pth'))\n",
    "D.load_state_dict(torch.load('weights/D3.pth'))\n",
    "res = G(img_variable)\n",
    "uv=res.cpu().detach().numpy()\n",
    "uv[:,0,:,:] *= 0.436\n",
    "uv[:,1,:,:] *= 0.615\n",
    "fr = np.concatenate([infimg,uv],axis=1).reshape(3,256,256)\n",
    "rgb=yuv2rgb(fr.transpose(1,2,0))\n",
    "cv2.imwrite('current.jpg',(rgb.clip(min=0,max=1)*256)[:,:,[2,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0: [D loss: 0.000000] [G loss: 24.361118]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 21.169556]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 21.799685]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 24.758905]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.759775]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.161333]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.299866]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 20.657955]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 21.871845]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.164768]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.579809]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 19.386799]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.742941]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.875778]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.054083]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 20.886280]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.602940]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.440645]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.391920]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 21.965626]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.720440]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.929249]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.173092]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.607481]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.268908]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.203735]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.280451]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.992168]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.788105]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.723129]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.895847]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 23.871576]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.383429]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.815435]\n",
      "Epoch: 0: [D loss: 0.000000] [G loss: 22.168423]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "flag = 0\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=5e-8, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=1e-6, betas=(0.5, 0.999))\n",
    "#G.load_state_dict(torch.load('model.pth'))\n",
    "#D.load_state_dict(torch.load('Dinit.pth'))\n",
    "for epoch in range(1000):\n",
    "    for y, uv in training_generator:\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.Tensor(y.size(0), 1).fill_(1.0), requires_grad=False).cuda()\n",
    "        fake = Variable(torch.Tensor(y.size(0), 1).fill_(0.0), requires_grad=False).cuda()\n",
    "\n",
    "        # Configure input\n",
    "        yvar = Variable(y).cuda()\n",
    "        real_imgs = torch.cat([yvar,Variable(uv).cuda()],dim=1)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = torch.cat([yvar.detach(),G(yvar)],dim=1)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(D(gen_imgs), valid)\n",
    "        if flag==0:\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(D(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        if flag>0:\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            print (\"Epoch: %d: [D loss: %f] [G loss: %f]\" % (epoch, d_loss.item(), g_loss.item()))\n",
    "        \n",
    "            torch.save(D.state_dict(), 'weights/D'+str(epoch)+'.pth')\n",
    "            torch.save(G.state_dict(), 'weights/G'+str(epoch)+'.pth')\n",
    "            res = G(img_variable)\n",
    "            uv=res.cpu().detach().numpy()\n",
    "            uv[:,0,:,:] *= 0.436\n",
    "            uv[:,1,:,:] *= 0.615\n",
    "            fr = np.concatenate([infimg,uv],axis=1).reshape(3,256,256)\n",
    "            rgb=yuv2rgb(fr.transpose(1,2,0))\n",
    "            cv2.imwrite('current.jpg',(rgb.clip(min=0,max=1)*256)[:,:,[2,1,0]])\n",
    "            flag = (flag+1)%6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
